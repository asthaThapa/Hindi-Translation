{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    LlamaForCausalLM, LlamaConfig, LlamaTokenizer,\n",
    "    Trainer, TrainingArguments, DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import sys\n",
    "import argparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_DEFINED_SYMBOLS = [\"<pad>\", \"<s>\", \"</s>\", \"<mask>\", \".\"]\n",
    "symbols = USER_DEFINED_SYMBOLS\n",
    "\n",
    "vowels = [\n",
    "    \"அ\", \"ஆ\", \"இ\", \"ஈ\", \"உ\", \"ஊ\", \"எ\", \"ஏ\", \"ஐ\", \"ஒ\", \"ஓ\", \"ஔ\", \"ஃ\"\n",
    "]\n",
    "consonants = [\n",
    "    \"க\", \"ங\", \"ச\", \"ஞ\", \"ட\", \"ண\", \"த\", \"ந\", \"ப\", \"ம\", \"ய\", \"ர\", \"ற\",\n",
    "    \"ன\", \"ல\", \"ள\", \"ழ\", \"வ\", \"ஷ\", \"ஸ\", \"ஹ\", \"க்ஷ\", \"ஜ\", \"ஶ\", \"ஸ்ரீ\"\n",
    "]\n",
    "dependent_vowels = [\n",
    "    \"ா\", \"ி\", \"ீ\", \"ு\", \"ூ\", \"ெ\", \"ே\", \"ை\", \"ொ\", \"ோ\", \"ௌ\", \"்\"\n",
    "]\n",
    "# merge all letters into a single array\n",
    "symbols.extend(vowels + consonants + dependent_vowels)\n",
    "print(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(language, input_path, model_prefix, vocab_size):\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=input_path,\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=vocab_size,\n",
    "        user_defined_symbols=language,\n",
    "        model_type=\"BPE\"\n",
    "    )\n",
    "\n",
    "def move_tokenizer_to_folder(source, destination_folder):\n",
    "    os.rename(source, os.path.join(destination_folder, \"tokenizer.model\"))\n",
    "\n",
    "def create_config_file(folder_path, content):\n",
    "    with open(os.path.join(folder_path, \"config.json\"), \"w\") as config_file:\n",
    "        json.dump(content, config_file, indent=4)\n",
    "\n",
    "config_content = {\n",
    "    \"_name_or_path\": \"./names_1m\",\n",
    "    \"architectures\": [\n",
    "        \"LlamaForCausalLM\"\n",
    "    ],\n",
    "    \"bos_token_id\": 2,\n",
    "    \"eos_token_id\": 3,\n",
    "    \"hidden_act\": \"silu\",\n",
    "    \"hidden_size\": 64,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 180,\n",
    "    \"max_position_embeddings\": 32,\n",
    "    \"model_type\": \"llama\",\n",
    "    \"num_attention_heads\": 16,\n",
    "    \"num_hidden_layers\": 8,\n",
    "    \"num_key_value_heads\": 16,\n",
    "    \"pad_token_id\": 1,\n",
    "    \"pretraining_tp\": 1,\n",
    "    \"rms_norm_eps\": 1e-06,\n",
    "    \"rope_scaling\": None,\n",
    "    \"tie_word_embeddings\": False,\n",
    "    \"torch_dtype\": \"float32\",\n",
    "    \"transformers_version\": \"4.28.1\",\n",
    "    \"use_cache\": False,\n",
    "    \"vocab_size\": 58\n",
    "}\n",
    "\n",
    "out_folder_path = \"names\"\n",
    "os.makedirs(out_folder_path, exist_ok=True)\n",
    "create_config_file(out_folder_path, config_content)\n",
    "train_tokenizer(symbols, 'data.txt', 'tokenizer', 58)\n",
    "move_tokenizer_to_folder(\"tokenizer.model\", out_folder_path)\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(out_folder_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_sentence = \"தமிழ் வாழ்க\"\n",
    "tokens = tokenizer(\n",
    "                sample_sentence, truncation=True,\n",
    "                padding='max_length', max_length=16)\n",
    "print(f\"Original Sentence: {sample_sentence}\\nTokenized Sentence: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config_model(path):\n",
    "    config = LlamaConfig.from_pretrained(path)\n",
    "\n",
    "    model = LlamaForCausalLM(config)\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model_size = sum(t.numel() for t in model.parameters())\n",
    "    print(f\"GPT Model size: {model_size/1000**2:.1f}M parameters\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_tokenized_dataset_splits(path, tokenizer, block_size):\n",
    "    dataset = load_dataset('text', data_files=path)\n",
    "    shuffled_dataset = dataset['train'].shuffle(seed=42)\n",
    "    split_datasets = shuffled_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "    def tokenize_dataset(dataset):\n",
    "        return dataset.map(\n",
    "            lambda examples: tokenizer(\n",
    "                examples['text'], truncation=True,\n",
    "                padding='max_length', max_length=block_size\n",
    "            ),\n",
    "            batched=True\n",
    "        )\n",
    "\n",
    "    def unique_name_set(dataset):\n",
    "      names_set = set()\n",
    "\n",
    "      for example in dataset:\n",
    "          name = example['text'].split(\".\")[0]\n",
    "          names_set.add(name)\n",
    "\n",
    "      return names_set\n",
    "\n",
    "    return tokenize_dataset(split_datasets['train']), tokenize_dataset(split_datasets['test']), unique_name_set(split_datasets['train'])\n",
    "\n",
    "def train_model(model, tokenizer, train_dataset, test_dataset, out_folder_path):\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_folder_path,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=100,\n",
    "        per_device_train_batch_size=8,\n",
    "        save_steps=10000,\n",
    "        logging_steps=10,\n",
    "        eval_steps=1000,\n",
    "        logging_dir=f'{out_folder_path}/logs',\n",
    "        evaluation_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.001)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(out_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_config_model(out_folder_path)\n",
    "train_dataset, test_dataset, unique_names = create_tokenized_dataset_splits('data.txt', tokenizer, block_size=32)\n",
    "train_model(model, tokenizer, train_dataset, test_dataset, out_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_names(model, tokenizer, prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    attention_mask = torch.ones_like(input_ids).to(model.device)\n",
    "    generated_names = set()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while len(generated_names) < 20:\n",
    "            output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=32,\n",
    "                early_stopping=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.8,\n",
    "                top_k=50,\n",
    "                do_sample=True,\n",
    "                output_scores=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.4,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            output_str = tokenizer.decode(output[0], skip_special_tokens=True).split(\".\")[0]\n",
    "            if output_str not in generated_names and output_str not in unique_names:\n",
    "                print(output_str)\n",
    "                generated_names.add(output_str)\n",
    "\n",
    "male_names_prompt = \"ஆண்,\"\n",
    "female_names_prompt = \"பெண்,\"\n",
    "\n",
    "model.eval()\n",
    "generate_names(model, tokenizer, male_names_prompt)\n",
    "generate_names(model, tokenizer, female_names_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
